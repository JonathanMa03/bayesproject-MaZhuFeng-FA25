---
title: "Model Building"
author: "Jonathan Ma"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: true
    number_sections: true
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, fig.width=6, fig.height=4)
library(tidyverse)
library(brms)
```

# Loading the Data In

We double check that data structure is correct, pull a specific cancer to analyse (breast in this case), and reformat anything so our model runs smoothly.

```{r Load and Prep}
df <- read.csv("../data/clean/seer_nov13.csv", sep = ",", header = TRUE, stringsAsFactors = TRUE)
str(df)
```

```{r Cancer choice}
cancer_site <- df %>% filter(site == 50) #breast cancer
str(cancer_site)
```

```{r Reformat}
seer_df <- cancer_site %>% 
  mutate(
    # Convert outcome: 1 = late-stage, 0 = early-stage
    latestage = case_when(
      stage %in% c("Regional", "Distant") ~ 1,
      stage == "Localized" ~ 0,
      TRUE ~ NA_real_
    ),
    # Convert IDs to factor
    patientid = as.factor(patientid),
    regionid = as.factor(regionid)
  ) %>%
  # Drop rows with missing outcome or covariates (optional)
  drop_na(latestage, grade, size, age, sex, raceth, marry, year)

seer_df$latestage <- as.integer(seer_df$latestage)

# drop unused columns
seer_df <- seer_df %>%
  select(-id, -region, -site, -stage)

str(seer_df)
```

# Model Initialization with Prior

We will sample with No U-Turn Sampling (NUTS). We assigned weakly informative priors to all model parameters. Fixed effects received independent Normal(0, 2²) priors, encouraging shrinkage without overly restricting plausible values. The intercept was assigned a wider Normal(0, 5²) prior to reflect uncertainty in baseline log-odds. Group-level standard deviations (for patientid and regionid) used Student-t(3, 0, 2.5) priors, which provide regularization while allowing for potential group-level variability.

## Model Specification

Logistic regression with:

- Outcome: `latestage` (1 = regional/distant, 0 = localized)
- Fixed effects: `age`, `sex`, `raceth`, `grade`, `size_z`, `year_z`, `marry`
- Random intercepts:
    - Patient-level: `(1 | patientid)`
    - Region-level: `(1 | regionid)`
- Estimation: NUTS via `cmdstanr` backend
- Priors:
    - $\beta_j \sim \mathcal{N}(0, 2^2)$
    - Intercept $\sim \mathcal{N}(0, 5^2)$
    - Group SDs $\sim t_3(0, 2.5)$ (approx IG)

```{r Model}
priors <- c(
  prior(normal(0, 2), class = "b"), # Fixed effects ~ Normal(0, 2^2)
  prior(normal(0, 5), class = "Intercept"), # Intercept ~ Normal(0, 5^2)
  prior(student_t(3, 0, 2.5), class = "sd") # Group-level SDs ~ Student-t(3, 0, 2.5) ≈ weak InvGamma
)

set.seed(632)  

# Build 3-level hierarchical model
brm_model <- brm(
  formula = latestage ~ age + sex + raceth + grade + size + year + marry +
    (1 | patientid) + (1 | regionid),
  data = seer_df,
  family = bernoulli(link = "logit"),
  prior=priors,
  backend = "cmdstanr",
  warmup = 1000,
  iter = 2000,               
  chains = 4,             # markov chains
  cores = 4, 
  control = list(adapt_delta = 0.99, max_treedepth = 15),
  seed = 632,
  refresh = 2000 #suppress output
)

# For Stan Backend
#stancode(brm_model)
```

## Model Fine Tuning

Increasing Warmpup to 1500 and 3000 iterations, we got divergence on 1/6000 transitions (0.017%), and 2 chains haveing E-BFMI (Energy Bayesian Fraction of Missing Information) less than 0.3 which is a diagnostic of poor posterior exploration which usually happens due to poor scaling. To counter this, we will standardize size and year variables, and increase stepsize to 0.9995 to avoid divergences.

```{r Fine Tune}
seer_df2 <- seer_df %>%
  mutate(
    size_z = scale(size),
    year_z = scale(year)
  )

set.seed(632)  

brm_model2 <- brm(formula = latestage ~ age + sex + raceth + grade + size_z + year_z + marry +
    (1 | patientid) + (1 | regionid),
  data = seer_df2,
  family = bernoulli(link = "logit"), prior=priors, backend = "cmdstanr",
  warmup = 1500, iter = 3000, chains = 4, cores = 4, 
  control = list(adapt_delta = 0.9995, max_treedepth = 15), init = 0,
  seed = 632, refresh = 3000 #suppress output
)

# write.csv(seer_df2, "../data/clean/seer_df2.csv", row.names = FALSE)
saveRDS(brm_model2, "../R/breast_model.rds")
```